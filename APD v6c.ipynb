{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DYNAMIC Peak count and depolarization start times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 6b changelist: \n",
    "- Per ROI instead of global median ROI\n",
    "- Added troubleshooting functionalities (grey in plot is detected but filtered out of data table)\n",
    "#\n",
    "TO DO:\n",
    "- Fix occasional drifts\n",
    "- Fix low amplitude peak detection, occasional\n",
    "- Maybe start_idx (depolarization start) needs better detection settings?\n",
    "QUESTIONS?: huynh.trung@mayo.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED PHOTOBLEACHING CORRECTION FOR CARDIOLAMINOPATHY iPSC-CM STUDIES\n",
    "# ============================================================================\n",
    "# Key fixes:\n",
    "# 1. Proper array length handling\n",
    "# 2. Robust trend interpolation\n",
    "# 3. Better baseline preservation\n",
    "# 4. Enhanced error recovery\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import optimize\n",
    "from itertools import groupby\n",
    "\n",
    "def robust_polynomial_correction_fixed(time, signal, degree=2):\n",
    "    \"\"\"\n",
    "    COMPLETELY FIXED polynomial correction with proper array handling.\n",
    "    \n",
    "    Key fixes:\n",
    "    - Proper array length matching\n",
    "    - Robust interpolation for missing values\n",
    "    - Better baseline preservation\n",
    "    - Numerical stability improvements\n",
    "    \n",
    "    References:\n",
    "    - Shinnawi et al. (2015) Nat Protoc - iPSC-CM calcium imaging\n",
    "    - Yang et al. (2014) Nat Protoc - photobleaching correction methods\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time = np.array(time, dtype=float)\n",
    "        signal = np.array(signal, dtype=float)\n",
    "        \n",
    "        # Ensure equal lengths\n",
    "        min_len = min(len(time), len(signal))\n",
    "        time = time[:min_len]\n",
    "        signal = signal[:min_len]\n",
    "        \n",
    "        print(f\"   Processing {len(signal)} points with polynomial degree {degree}\")\n",
    "        \n",
    "        # Remove invalid values\n",
    "        valid_mask = np.isfinite(time) & np.isfinite(signal)\n",
    "        n_valid = np.sum(valid_mask)\n",
    "        \n",
    "        if n_valid < degree + 1:\n",
    "            print(f\"   ❌ Insufficient valid points: {n_valid} < {degree + 1}\")\n",
    "            return signal, np.full_like(signal, np.nanmean(signal)), 'insufficient_data'\n",
    "        \n",
    "        time_clean = time[valid_mask]\n",
    "        signal_clean = signal[valid_mask]\n",
    "        \n",
    "        # THE CRITICAL FIX: Proper time normalization\n",
    "        time_range = np.ptp(time_clean)\n",
    "        if time_range == 0:\n",
    "            print(\"   ❌ Time array has zero range\")\n",
    "            return signal, np.full_like(signal, np.nanmean(signal)), 'zero_time_range'\n",
    "        \n",
    "        time_mean = np.mean(time_clean)\n",
    "        time_normalized = (time_clean - time_mean) / time_range\n",
    "        \n",
    "        # Check for numerical stability\n",
    "        condition_number = np.max(np.abs(time_normalized)) ** degree\n",
    "        if condition_number > 1e10:\n",
    "            print(f\"   ⚠️ Poor conditioning detected, reducing to linear fit\")\n",
    "            degree = 1\n",
    "            \n",
    "        # Fit polynomial with proper error handling\n",
    "        try:\n",
    "            coeffs = np.polyfit(time_normalized, signal_clean, degree, \n",
    "                              rcond=None, full=False, w=None, cov=False)\n",
    "            trend_clean = np.polyval(coeffs, time_normalized)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            print(f\"   ❌ Polynomial fit failed: {e}, falling back to linear\")\n",
    "            if len(time_clean) >= 2:\n",
    "                slope = (signal_clean[-1] - signal_clean[0]) / (time_clean[-1] - time_clean[0])\n",
    "                trend_clean = signal_clean[0] + slope * (time_clean - time_clean[0])\n",
    "            else:\n",
    "                trend_clean = np.full_like(signal_clean, np.mean(signal_clean))\n",
    "        \n",
    "        # CRITICAL FIX: Proper interpolation back to original grid\n",
    "        trend_full = np.full_like(signal, np.nan)\n",
    "        trend_full[valid_mask] = trend_clean\n",
    "        \n",
    "        # Interpolate missing values\n",
    "        if np.any(~valid_mask):\n",
    "            valid_indices = np.where(valid_mask)[0]\n",
    "            invalid_indices = np.where(~valid_mask)[0]\n",
    "            \n",
    "            if len(valid_indices) >= 2:\n",
    "                trend_full[invalid_indices] = np.interp(\n",
    "                    time[invalid_indices], \n",
    "                    time[valid_indices], \n",
    "                    trend_full[valid_indices]\n",
    "                )\n",
    "            else:\n",
    "                # Fill with mean if insufficient data for interpolation\n",
    "                trend_full[invalid_indices] = np.nanmean(trend_full[valid_indices])\n",
    "        \n",
    "        # Handle any remaining NaN values\n",
    "        if np.any(np.isnan(trend_full)):\n",
    "            trend_full = np.nan_to_num(trend_full, nan=np.nanmean(trend_full))\n",
    "        \n",
    "        # FIXED: Proper baseline preservation\n",
    "        # Preserve the original baseline (minimum value region)\n",
    "        baseline_original = np.nanmin(signal)\n",
    "        baseline_trend = np.nanmin(trend_full)\n",
    "        \n",
    "        # Correct for photobleaching while preserving baseline\n",
    "        corrected = signal - (trend_full - baseline_trend)\n",
    "        \n",
    "        # Quality assessment\n",
    "        if len(signal_clean) > 1:\n",
    "            ss_res = np.sum((signal_clean - trend_clean) ** 2)\n",
    "            ss_tot = np.sum((signal_clean - np.mean(signal_clean)) ** 2)\n",
    "            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "            print(f\"   ✅ Polynomial correction: R² = {r_squared:.3f}, degree = {degree}\")\n",
    "        else:\n",
    "            r_squared = 0\n",
    "            \n",
    "        return corrected, trend_full, f'polynomial_deg{degree}'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Polynomial correction failed: {e}\")\n",
    "        return _linear_fallback(time, signal)\n",
    "\n",
    "def _linear_fallback(time, signal):\n",
    "    \"\"\"Robust linear fallback for when polynomial fails.\"\"\"\n",
    "    try:\n",
    "        valid_mask = np.isfinite(time) & np.isfinite(signal)\n",
    "        \n",
    "        if np.sum(valid_mask) < 2:\n",
    "            print(\"   ❌ Insufficient valid data for linear fit\")\n",
    "            return signal, np.zeros_like(signal), 'failed'\n",
    "        \n",
    "        time_valid = time[valid_mask]\n",
    "        signal_valid = signal[valid_mask]\n",
    "        \n",
    "        if len(np.unique(time_valid)) < 2:\n",
    "            print(\"   ❌ Non-unique time values\")\n",
    "            return signal, np.full_like(signal, np.mean(signal_valid)), 'constant'\n",
    "        \n",
    "        # Simple linear regression\n",
    "        slope = (signal_valid[-1] - signal_valid[0]) / (time_valid[-1] - time_valid[0])\n",
    "        intercept = signal_valid[0] - slope * time_valid[0]\n",
    "        \n",
    "        trend = slope * time + intercept\n",
    "        corrected = signal - (trend - trend[0])\n",
    "        \n",
    "        print(\"   ✅ Linear fallback successful\")\n",
    "        return corrected, trend, 'linear_fallback'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Even linear fallback failed: {e}\")\n",
    "        return signal, np.zeros_like(signal), 'failed'\n",
    "\n",
    "def smart_photobleach_correction_fixed(time, signal, method='auto'):\n",
    "    \"\"\"\n",
    "    COMPLETELY FIXED photobleaching correction for iPSC-CM calcium transients.\n",
    "    \n",
    "    Major improvements:\n",
    "    - Fixed array length mismatches\n",
    "    - Robust interpolation handling\n",
    "    - Better baseline preservation\n",
    "    - Enhanced method selection for cardiolaminopathy studies\n",
    "    \n",
    "    References:\n",
    "    - Shinnawi et al. (2015) Nat Protoc 10:1889-1902\n",
    "    - Yang et al. (2014) Nat Protoc 9:1028-1037\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation and cleaning\n",
    "    signal = np.array(signal, dtype=float)\n",
    "    time = np.array(time, dtype=float)\n",
    "    \n",
    "    # Ensure equal lengths (CRITICAL FIX)\n",
    "    min_len = min(len(time), len(signal))\n",
    "    if min_len != len(time) or min_len != len(signal):\n",
    "        print(f\"   ⚠️ Array length mismatch: time={len(time)}, signal={len(signal)}\")\n",
    "        time = time[:min_len]\n",
    "        signal = signal[:min_len]\n",
    "    \n",
    "    print(f\"   Input: {len(signal)} points\")\n",
    "    \n",
    "    # Remove invalid values\n",
    "    valid_mask = np.isfinite(signal) & np.isfinite(time)\n",
    "    n_valid = np.sum(valid_mask)\n",
    "    \n",
    "    if n_valid < 10:\n",
    "        print(f\"   ❌ Insufficient valid data: {n_valid} points\")\n",
    "        return signal, np.zeros_like(signal), 'insufficient_data'\n",
    "    \n",
    "    if not np.all(valid_mask):\n",
    "        print(f\"   ⚠️ Removed {len(signal) - n_valid} invalid points\")\n",
    "    \n",
    "    # Ensure monotonic time (common issue with experimental data)\n",
    "    if not np.all(np.diff(time[valid_mask]) > 0):\n",
    "        print(\"   ⚠️ Non-monotonic time detected, sorting data\")\n",
    "        sort_indices = np.argsort(time)\n",
    "        time = time[sort_indices]\n",
    "        signal = signal[sort_indices]\n",
    "        # Recalculate valid mask after sorting\n",
    "        valid_mask = np.isfinite(signal) & np.isfinite(time)\n",
    "    \n",
    "    # Signal characterization for method selection\n",
    "    signal_valid = signal[valid_mask]\n",
    "    time_valid = time[valid_mask]\n",
    "    \n",
    "    signal_range = np.ptp(signal_valid)\n",
    "    noise_level = np.std(np.diff(signal_valid)) * np.sqrt(2)\n",
    "    snr = signal_range / noise_level if noise_level > 0 else 1000\n",
    "    \n",
    "    # Photobleaching severity assessment\n",
    "    n_segments = min(10, len(signal_valid) // 20)\n",
    "    if n_segments >= 2:\n",
    "        segment_size = len(signal_valid) // n_segments\n",
    "        start_mean = np.mean(signal_valid[:segment_size])\n",
    "        end_mean = np.mean(signal_valid[-segment_size:])\n",
    "        bleaching_severity = abs(start_mean - end_mean) / signal_range if signal_range > 0 else 0\n",
    "    else:\n",
    "        bleaching_severity = 0\n",
    "    \n",
    "    print(f\"   Signal characteristics: SNR={snr:.1f}, bleaching={bleaching_severity:.1%}\")\n",
    "    \n",
    "    # Enhanced method selection for iPSC-CM calcium imaging\n",
    "    if method == 'auto':\n",
    "        if len(signal_valid) < 20:\n",
    "            method = 'linear'\n",
    "        elif bleaching_severity < 0.03:  # Very minimal bleaching\n",
    "            method = 'none'\n",
    "        elif snr < 2:  # Very noisy data\n",
    "            method = 'rolling'\n",
    "        elif bleaching_severity > 0.2 and len(signal_valid) > 100:  # Severe bleaching\n",
    "            method = 'exponential'\n",
    "        elif len(signal_valid) > 50 and snr > 3:  # Good conditions\n",
    "            method = 'polynomial'\n",
    "        else:\n",
    "            method = 'linear'\n",
    "    \n",
    "    print(f\"   Selected method: {method}\")\n",
    "    \n",
    "    # Apply correction with proper error handling\n",
    "    try:\n",
    "        if method == 'none':\n",
    "            corrected = signal.copy()\n",
    "            trend = np.full_like(signal, np.nanmean(signal_valid))\n",
    "            used_method = 'none'\n",
    "            \n",
    "        elif method == 'linear':\n",
    "            corrected, trend, used_method = _apply_linear_correction(time, signal, valid_mask)\n",
    "            \n",
    "        elif method == 'polynomial':\n",
    "            corrected, trend, used_method = robust_polynomial_correction_fixed(time, signal, degree=2)\n",
    "            \n",
    "        elif method == 'rolling':\n",
    "            corrected, trend, used_method = _apply_rolling_correction(time, signal, valid_mask)\n",
    "            \n",
    "        elif method == 'exponential':\n",
    "            corrected, trend, used_method = _apply_exponential_correction(time, signal, valid_mask)\n",
    "            \n",
    "        else:\n",
    "            # Default to linear\n",
    "            corrected, trend, used_method = _apply_linear_correction(time, signal, valid_mask)\n",
    "        \n",
    "        # Final validation\n",
    "        if np.any(np.isnan(corrected)) or np.any(np.isinf(corrected)):\n",
    "            print(\"   ❌ Correction produced invalid values, using original\")\n",
    "            corrected = signal.copy()\n",
    "            trend = np.full_like(signal, np.nanmean(signal_valid))\n",
    "            used_method = 'failed_validation'\n",
    "        \n",
    "        # Report effectiveness\n",
    "        correction_magnitude = np.ptp(trend) / signal_range if signal_range > 0 else 0\n",
    "        print(f\"   ✅ Correction applied: {correction_magnitude:.1%} of signal range\")\n",
    "        \n",
    "        return corrected, trend, used_method\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ All correction methods failed: {e}\")\n",
    "        return signal, np.zeros_like(signal), 'complete_failure'\n",
    "\n",
    "def _apply_linear_correction(time, signal, valid_mask):\n",
    "    \"\"\"Apply robust linear detrending.\"\"\"\n",
    "    try:\n",
    "        time_valid = time[valid_mask]\n",
    "        signal_valid = signal[valid_mask]\n",
    "        \n",
    "        if len(time_valid) < 2:\n",
    "            return signal, np.zeros_like(signal), 'insufficient_linear_data'\n",
    "        \n",
    "        # Linear regression\n",
    "        time_range = time_valid[-1] - time_valid[0]\n",
    "        if time_range == 0:\n",
    "            slope = 0\n",
    "        else:\n",
    "            slope = (signal_valid[-1] - signal_valid[0]) / time_range\n",
    "        \n",
    "        intercept = signal_valid[0] - slope * time_valid[0]\n",
    "        trend = slope * time + intercept\n",
    "        \n",
    "        # Preserve baseline\n",
    "        corrected = signal - (trend - trend[0])\n",
    "        \n",
    "        return corrected, trend, 'linear'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Linear correction failed: {e}\")\n",
    "        return signal, np.zeros_like(signal), 'linear_failed'\n",
    "\n",
    "def _apply_rolling_correction(time, signal, valid_mask, window_percent=15):\n",
    "    \"\"\"Apply rolling baseline correction.\"\"\"\n",
    "    try:\n",
    "        signal_valid = signal[valid_mask]\n",
    "        \n",
    "        window_size = max(5, len(signal_valid) // (100 // window_percent))\n",
    "        \n",
    "        # Rolling percentile baseline\n",
    "        baseline = np.array([\n",
    "            np.percentile(signal[max(0, i-window_size//2):min(len(signal), i+window_size//2)], 5)\n",
    "            for i in range(len(signal))\n",
    "        ])\n",
    "        \n",
    "        # Smooth the baseline\n",
    "        if len(baseline) > 10:\n",
    "            baseline = gaussian_filter1d(baseline, sigma=max(1, window_size//10))\n",
    "        \n",
    "        # Preserve original baseline level\n",
    "        corrected = signal - (baseline - baseline[0])\n",
    "        \n",
    "        return corrected, baseline, 'rolling'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Rolling correction failed: {e}\")\n",
    "        return _apply_linear_correction(time, signal, valid_mask)\n",
    "\n",
    "def _apply_exponential_correction(time, signal, valid_mask):\n",
    "    \"\"\"Apply exponential decay correction for severe photobleaching.\"\"\"\n",
    "    try:\n",
    "        time_valid = time[valid_mask]\n",
    "        signal_valid = signal[valid_mask]\n",
    "        \n",
    "        if len(signal_valid) < 10:\n",
    "            return _apply_linear_correction(time, signal, valid_mask)\n",
    "        \n",
    "        # Find baseline regions (low variance windows)\n",
    "        window_size = max(10, len(signal_valid) // 20)\n",
    "        baseline_indices = []\n",
    "        \n",
    "        for i in range(0, len(signal_valid) - window_size, window_size//2):\n",
    "            window = signal_valid[i:i + window_size]\n",
    "            if np.var(window) < np.var(signal_valid) * 0.1:  # Low variance region\n",
    "                baseline_indices.extend(range(i, i + window_size))\n",
    "        \n",
    "        if len(baseline_indices) < 5:\n",
    "            return _apply_linear_correction(time, signal, valid_mask)\n",
    "        \n",
    "        # Fit exponential decay\n",
    "        baseline_times = time_valid[baseline_indices]\n",
    "        baseline_values = signal_valid[baseline_indices]\n",
    "        \n",
    "        def exp_func(t, a, b, c):\n",
    "            return a * np.exp(-b * (t - t[0])) + c\n",
    "        \n",
    "        # Initial parameter estimates\n",
    "        p0 = [\n",
    "            np.max(baseline_values) - np.min(baseline_values),\n",
    "            0.01,\n",
    "            np.min(baseline_values)\n",
    "        ]\n",
    "        \n",
    "        bounds = ([0, 0, -np.inf], [np.inf, 1, np.inf])\n",
    "        \n",
    "        popt, _ = optimize.curve_fit(\n",
    "            exp_func, baseline_times, baseline_values,\n",
    "            p0=p0, bounds=bounds, maxfev=1000\n",
    "        )\n",
    "        \n",
    "        trend = exp_func(time, *popt)\n",
    "        corrected = signal - (trend - trend[0])\n",
    "        \n",
    "        print(f\"   ✅ Exponential fit: decay = {popt[1]:.4f}\")\n",
    "        return corrected, trend, 'exponential'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Exponential correction failed: {e}\")\n",
    "        return _apply_linear_correction(time, signal, valid_mask)\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED ANALYSIS FUNCTION WITH FIXED CORRECTION\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_roi_signal_with_fixed_correction(time, signal, sample_name, roi_label,\n",
    "                                           apply_filters=True,\n",
    "                                           grey_out_unfiltered=True,\n",
    "                                           photobleach_method='auto',\n",
    "                                           band_factor=1.5,\n",
    "                                           upstroke_min=0.06):\n",
    "    \"\"\"\n",
    "    Complete APD analysis with FIXED photobleaching correction.\n",
    "    Optimized for cardiolaminopathy iPSC-CM studies.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🔍 Analyzing {sample_name} | {roi_label}\")\n",
    "        \n",
    "        # Data preparation\n",
    "        signal = pd.to_numeric(signal, errors='coerce')\n",
    "        signal = signal.dropna().values\n",
    "        time = time[:len(signal)]\n",
    "        \n",
    "        if len(signal) < 10:\n",
    "            print(f\"   ❌ Signal too short: {len(signal)} points\")\n",
    "            return sample_name, roi_label, pd.DataFrame(), None\n",
    "        \n",
    "        # Handle non-positive values for ΔF/F calculation\n",
    "        Fmin = np.min(signal)\n",
    "        if Fmin <= 0:\n",
    "            signal = signal - Fmin + 0.001\n",
    "            Fmin = 0.001\n",
    "        \n",
    "        normalized = (signal - Fmin) / Fmin\n",
    "        \n",
    "        # Apply FIXED photobleaching correction\n",
    "        print(f\"   📈 Applying photobleaching correction...\")\n",
    "        corrected, trend, method_used = smart_photobleach_correction_fixed(\n",
    "            time, normalized, method=photobleach_method\n",
    "        )\n",
    "        \n",
    "        # Smooth the corrected signal\n",
    "        smoothed = gaussian_filter1d(corrected, sigma=1)\n",
    "        \n",
    "        # Peak detection\n",
    "        print(f\"   🔍 Detecting calcium transients...\")\n",
    "        s_max, s_med = np.max(smoothed), np.median(smoothed)\n",
    "        dr = s_max - s_med\n",
    "        \n",
    "        if dr < 0.001:\n",
    "            print(f\"   ❌ Signal too flat for peak detection\")\n",
    "            return sample_name, roi_label, pd.DataFrame(), None\n",
    "        \n",
    "        # Detection thresholds\n",
    "        rising_thresh = 0.05 * dr\n",
    "        depol_thresh = 0.02 * dr\n",
    "        slope = np.gradient(smoothed, time)\n",
    "        \n",
    "        # Find rising edges\n",
    "        edges = np.where(slope > rising_thresh)[0]\n",
    "        groups = []\n",
    "        for k, g in groupby(enumerate(edges), lambda x: x[0] - x[1]):\n",
    "            grp = [i for _, i in g]\n",
    "            if len(grp) >= 5:\n",
    "                groups.append(grp)\n",
    "        \n",
    "        # Find peaks\n",
    "        peaks = []\n",
    "        for grp in groups:\n",
    "            start = grp[0]\n",
    "            end = min(grp[-1] + 20, len(smoothed))\n",
    "            peak_idx = np.argmax(smoothed[start:end]) + start\n",
    "            peaks.append(peak_idx)\n",
    "        \n",
    "        # De-duplicate peaks\n",
    "        filtered_peaks = []\n",
    "        for p in peaks:\n",
    "            if not filtered_peaks or p - filtered_peaks[-1] > 40:\n",
    "                filtered_peaks.append(p)\n",
    "        \n",
    "        if not filtered_peaks:\n",
    "            print(f\"   ❌ No peaks detected\")\n",
    "            return sample_name, roi_label, pd.DataFrame(), None\n",
    "        \n",
    "        print(f\"   ✅ Found {len(filtered_peaks)} calcium transients\")\n",
    "        \n",
    "        # Calculate APD metrics\n",
    "        rows = []\n",
    "        for pk in filtered_peaks:\n",
    "            try:\n",
    "                # Find depolarization start\n",
    "                window = range(max(0, pk - 100), pk)\n",
    "                candidates = [i for i in window if slope[i] < depol_thresh]\n",
    "                if not candidates:\n",
    "                    continue\n",
    "                start_idx = candidates[-1]\n",
    "                \n",
    "                # Calculate baseline and amplitude\n",
    "                pre_window = smoothed[max(0, start_idx - 50): start_idx]\n",
    "                baseline = np.min(pre_window) if len(pre_window) > 0 else smoothed[start_idx]\n",
    "                peak_val = smoothed[pk]\n",
    "                amplitude = peak_val - baseline\n",
    "                \n",
    "                if amplitude < 0.001:\n",
    "                    continue\n",
    "                \n",
    "                # APD90 (10% repolarization)\n",
    "                level_90 = baseline + 0.1 * amplitude\n",
    "                repol_90_idx = None\n",
    "                for i in range(pk + 1, len(smoothed)):\n",
    "                    if smoothed[i] <= level_90:\n",
    "                        repol_90_idx = i\n",
    "                        break\n",
    "                \n",
    "                if repol_90_idx is None:\n",
    "                    continue\n",
    "                \n",
    "                # APD50 (50% repolarization)\n",
    "                level_50 = baseline + 0.5 * amplitude\n",
    "                repol_50_idx = None\n",
    "                for i in range(pk + 1, len(smoothed)):\n",
    "                    if smoothed[i] <= level_50:\n",
    "                        repol_50_idx = i\n",
    "                        break\n",
    "                \n",
    "                if repol_50_idx is None:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate durations\n",
    "                apd90 = time[repol_90_idx] - time[start_idx]\n",
    "                apd50 = time[repol_50_idx] - time[start_idx]\n",
    "                ratio_50_90 = apd50 / apd90 if apd90 > 0 else np.nan\n",
    "                upstroke = time[pk] - time[start_idx]\n",
    "                \n",
    "                rows.append({\n",
    "                    'Depolarization_Start_Time_s': time[start_idx],\n",
    "                    'Peak_Time_s': time[pk],\n",
    "                    'Amplitude': amplitude,\n",
    "                    'APD50_s': apd50,\n",
    "                    'APD90_s': apd90,\n",
    "                    'APD50_90_Ratio': ratio_50_90,\n",
    "                    'Upstroke_Time_s': upstroke,\n",
    "                    'Repolarization_Time_s': time[repol_90_idx],\n",
    "                    'Repolarization_Level': smoothed[repol_90_idx]\n",
    "                })\n",
    "                \n",
    "            except Exception as peak_error:\n",
    "                print(f\"   ⚠️ Peak analysis failed: {peak_error}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_res_raw = pd.DataFrame(rows)\n",
    "        df_res = df_res_raw.copy()\n",
    "        \n",
    "        if df_res.empty:\n",
    "            print(f\"   ❌ No valid APD measurements\")\n",
    "            return sample_name, roi_label, pd.DataFrame(), None\n",
    "        \n",
    "        # Apply filters\n",
    "        if apply_filters:\n",
    "            initial_count = len(df_res)\n",
    "            \n",
    "            # Upstroke filter\n",
    "            df_res = df_res[df_res['Upstroke_Time_s'] > upstroke_min]\n",
    "            \n",
    "            # APD90 filter (per ROI median-based)\n",
    "            if not df_res.empty:\n",
    "                median_apd90 = df_res['APD90_s'].median()\n",
    "                lower = median_apd90 / band_factor\n",
    "                upper = median_apd90 * band_factor\n",
    "                df_res = df_res[(df_res['APD90_s'] >= lower) & (df_res['APD90_s'] <= upper)]\n",
    "            \n",
    "            df_res = df_res.reset_index(drop=True)\n",
    "            final_count = len(df_res)\n",
    "            \n",
    "            print(f\"   📊 Filters applied: {initial_count} → {final_count} events\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Top panel: Photobleaching correction\n",
    "        ax1.plot(time, normalized, 'b-', alpha=0.6, linewidth=1, label='Original ΔF/F')\n",
    "        ax1.plot(time, trend, 'r--', alpha=0.8, linewidth=2, label=f'Trend ({method_used})')\n",
    "        ax1.plot(time, corrected, 'g-', alpha=0.9, linewidth=1.5, label='Corrected')\n",
    "        ax1.set_ylabel(\"ΔF/F₀\")\n",
    "        ax1.set_title(f\"{sample_name} | {roi_label} - Photobleaching Correction\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom panel: APD analysis\n",
    "        ax2.plot(time, smoothed, 'k-', alpha=0.8, linewidth=1.5, label='Smoothed Signal')\n",
    "        \n",
    "        # Plot events\n",
    "        if not df_res.empty:\n",
    "            for i, row in enumerate(df_res.itertuples()):\n",
    "                ax2.axvline(row.Peak_Time_s, color='red', linestyle='--', alpha=0.8,\n",
    "                           label='Peak' if i == 0 else None)\n",
    "                ax2.axvline(row.Depolarization_Start_Time_s, color='blue', linestyle=':', alpha=0.6,\n",
    "                           label='Depol Start' if i == 0 else None)\n",
    "                ax2.hlines(y=row.Repolarization_Level,\n",
    "                          xmin=row.Depolarization_Start_Time_s,\n",
    "                          xmax=row.Repolarization_Time_s,\n",
    "                          color='orange', linestyle='-', linewidth=3, alpha=0.8,\n",
    "                          label='APD90' if i == 0 else None)\n",
    "        \n",
    "        # Show filtered events in grey\n",
    "        if grey_out_unfiltered and apply_filters and not df_res_raw.empty:\n",
    "            filtered_out = df_res_raw[~df_res_raw.index.isin(df_res.index)]\n",
    "            for _, row in filtered_out.iterrows():\n",
    "                try:\n",
    "                    pk_time = row['Peak_Time_s']\n",
    "                    start_time = row['Depolarization_Start_Time_s'] \n",
    "                    repol_time = row['Repolarization_Time_s']\n",
    "                    repol_level = row['Repolarization_Level']\n",
    "                    \n",
    "                    ax2.axvline(pk_time, color='grey', linestyle='--', alpha=0.3)\n",
    "                    ax2.hlines(y=repol_level, xmin=start_time, xmax=repol_time,\n",
    "                              color='grey', linestyle='-', linewidth=1, alpha=0.3)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        ax2.set_xlabel(\"Time (s)\")\n",
    "        ax2.set_ylabel(\"ΔF/F₀\")\n",
    "        ax2.set_title(f\"APD Analysis Results ({len(df_res)} valid events)\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return sample_name, roi_label, df_res, fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Complete analysis failed: {e}\")\n",
    "        return sample_name, roi_label, pd.DataFrame(), None\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_fixed_analysis(excel_path, \n",
    "                      band_factor=1.5, \n",
    "                      upstroke_min=0.06,\n",
    "                      photobleach_method='auto',\n",
    "                      apply_filters=True):\n",
    "    \"\"\"\n",
    "    Run the complete fixed analysis workflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔧 FIXED APD Analysis for Cardiolaminopathy Studies\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📁 Input: {excel_path}\")\n",
    "    print(f\"🔬 Method: {photobleach_method}\")\n",
    "    print(f\"🎛️ Filters: {apply_filters}\")\n",
    "    \n",
    "    try:\n",
    "        xls = pd.ExcelFile(excel_path)\n",
    "        print(f\"✅ Loaded {len(xls.sheet_names)} sheets\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load Excel file: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    all_results = []\n",
    "    plots = []\n",
    "    failed_rois = []\n",
    "    \n",
    "    for sheet_name in xls.sheet_names:\n",
    "        print(f\"\\n📄 Processing: {sheet_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Parse sample name\n",
    "            raw = xls.parse(sheet_name, header=None, nrows=1)\n",
    "            if raw.dropna(how='all').empty:\n",
    "                continue\n",
    "            sample_name = str(raw.iloc[0, 0])\n",
    "            if sample_name.lower().endswith('.nd2'):\n",
    "                sample_name = sample_name[:-4]\n",
    "            \n",
    "            # Parse data\n",
    "            df = xls.parse(sheet_name, header=1)\n",
    "            if df.dropna(how='all').empty:\n",
    "                continue\n",
    "            df.columns = df.columns.astype(str).str.strip()\n",
    "            \n",
    "            time_cols = [c for c in df.columns if 'time' in c.lower()]\n",
    "            roi_cols = [c for c in df.columns if 'Mono' in c]\n",
    "            \n",
    "            if not time_cols or not roi_cols:\n",
    "                print(f\"   ❌ Missing required columns\")\n",
    "                continue\n",
    "            \n",
    "            time = pd.to_numeric(df[time_cols[0]], errors='coerce').values\n",
    "            print(f\"   📊 Found {len(roi_cols)} ROI columns\")\n",
    "            \n",
    "            # Process each ROI\n",
    "            for roi in roi_cols:\n",
    "                try:\n",
    "                    print(f\"      🔍 {roi}... \", end=\"\")\n",
    "                    \n",
    "                    sample, roi_label, res_df, fig = analyze_roi_signal_with_fixed_correction(\n",
    "                        time, df[roi], sample_name, roi,\n",
    "                        apply_filters=apply_filters,\n",
    "                        grey_out_unfiltered=True,\n",
    "                        photobleach_method=photobleach_method,\n",
    "                        band_factor=band_factor,\n",
    "                        upstroke_min=upstroke_min\n",
    "                    )\n",
    "                    \n",
    "                    if res_df.empty:\n",
    "                        print(\"❌ No events\")\n",
    "                        failed_rois.append((sample_name, roi, \"No events detected\"))\n",
    "                        if fig is not None:\n",
    "                            plt.close(fig)\n",
    "                        continue\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    res_df.insert(0, 'Sample', sample)\n",
    "                    res_df.insert(1, 'ROI', roi_label)\n",
    "                    \n",
    "                    all_results.append(res_df)\n",
    "                    if fig is not None:\n",
    "                        plots.append((sample, roi_label, fig))\n",
    "                    \n",
    "                    print(f\"✅ {len(res_df)} events\")\n",
    "                    \n",
    "                except Exception as roi_error:\n",
    "                    print(f\"❌ {roi_error}\")\n",
    "                    failed_rois.append((sample_name, roi, str(roi_error)))\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as sheet_error:\n",
    "            print(f\"   ❌ Sheet error: {sheet_error}\")\n",
    "            continue\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if all_results:\n",
    "        summary_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        print(f\"✅ SUCCESS:\")\n",
    "        print(f\"   • Samples: {summary_df['Sample'].nunique()}\")\n",
    "        print(f\"   • ROIs: {len(plots)}\")\n",
    "        print(f\"   • Total events: {len(summary_df)}\")\n",
    "        print(f\"   • Events/ROI: {len(summary_df)/len(plots):.1f}\")\n",
    "        \n",
    "        # Key metrics for cardiolaminopathy\n",
    "        if 'APD90_s' in summary_df.columns:\n",
    "            apd90_stats = summary_df['APD90_s'].describe()\n",
    "            print(f\"\\n📈 APD90 Statistics:\")\n",
    "            print(f\"   • Mean ± SD: {apd90_stats['mean']:.3f} ± {apd90_stats['std']:.3f} s\")\n",
    "            print(f\"   • Range: {apd90_stats['min']:.3f} - {apd90_stats['max']:.3f} s\")\n",
    "            \n",
    "        return summary_df, plots, failed_rois\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ NO SUCCESSFUL ANALYSES\")\n",
    "        return None, None, failed_rois\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION AND COMPARISON FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compare_correction_methods(time, signal, sample_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Compare different photobleaching correction methods.\n",
    "    Useful for troubleshooting and optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    methods = ['none', 'linear', 'polynomial', 'rolling', 'exponential']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original signal\n",
    "    axes[0].plot(time, signal, 'b-', alpha=0.8, linewidth=1.5)\n",
    "    axes[0].set_title(\"Original Signal\")\n",
    "    axes[0].set_ylabel(\"ΔF/F₀\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test each method\n",
    "    for i, method in enumerate(methods):\n",
    "        try:\n",
    "            corrected, trend, used_method = smart_photobleach_correction_fixed(\n",
    "                time, signal, method=method\n",
    "            )\n",
    "            \n",
    "            ax = axes[i + 1]\n",
    "            ax.plot(time, signal, 'b-', alpha=0.5, label='Original')\n",
    "            ax.plot(time, trend, 'r--', alpha=0.8, label='Trend')\n",
    "            ax.plot(time, corrected, 'g-', alpha=0.9, label='Corrected')\n",
    "            ax.set_title(f\"{method.upper()} → {used_method}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Calculate correction effectiveness\n",
    "            correction_magnitude = np.ptp(trend) / np.ptp(signal) if np.ptp(signal) > 0 else 0\n",
    "            ax.text(0.02, 0.98, f\"Correction: {correction_magnitude:.1%}\", \n",
    "                   transform=ax.transAxes, va='top', ha='left',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes[i + 1].text(0.5, 0.5, f\"FAILED\\n{method}\\n{e}\", \n",
    "                             transform=axes[i + 1].transAxes, \n",
    "                             ha='center', va='center')\n",
    "            axes[i + 1].set_title(f\"{method.upper()} - FAILED\")\n",
    "    \n",
    "    plt.suptitle(f\"Photobleaching Correction Comparison - {sample_name}\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def validate_apd_measurements(time, signal, results_df, sample_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Validate APD measurements with detailed visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results to validate\")\n",
    "        return None\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Top: Signal with all detected events\n",
    "    ax1.plot(time, signal, 'k-', alpha=0.8, linewidth=1.5, label='Signal')\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(results_df)))\n",
    "    \n",
    "    for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Mark key timepoints\n",
    "        peak_time = row['Peak_Time_s']\n",
    "        start_time = row['Depolarization_Start_Time_s']\n",
    "        repol_time = row['Repolarization_Time_s']\n",
    "        \n",
    "        ax1.axvline(peak_time, color=color, linestyle='--', alpha=0.8)\n",
    "        ax1.axvline(start_time, color=color, linestyle=':', alpha=0.6)\n",
    "        ax1.axvline(repol_time, color=color, linestyle='-.', alpha=0.6)\n",
    "        \n",
    "        # APD90 span\n",
    "        ax1.hlines(y=row['Repolarization_Level'], \n",
    "                  xmin=start_time, xmax=repol_time,\n",
    "                  color=color, linewidth=3, alpha=0.7)\n",
    "        \n",
    "        # Label\n",
    "        ax1.text(peak_time, signal[np.argmin(np.abs(time - peak_time))], \n",
    "                f'{i+1}', ha='center', va='bottom', \n",
    "                bbox=dict(boxstyle='circle', facecolor=color, alpha=0.7))\n",
    "    \n",
    "    ax1.set_ylabel(\"ΔF/F₀\")\n",
    "    ax1.set_title(f\"APD Measurements Validation - {sample_name}\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom: APD metrics\n",
    "    ax2.scatter(range(len(results_df)), results_df['APD90_s'], \n",
    "               c=colors, s=100, alpha=0.8, label='APD90')\n",
    "    ax2.scatter(range(len(results_df)), results_df['APD50_s'], \n",
    "               c=colors, s=60, alpha=0.6, marker='s', label='APD50')\n",
    "    \n",
    "    ax2.set_xlabel(\"Event Number\")\n",
    "    ax2.set_ylabel(\"APD (s)\")\n",
    "    ax2.set_title(\"APD Measurements\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f\"\"\"APD90: {results_df['APD90_s'].mean():.3f} ± {results_df['APD90_s'].std():.3f} s\n",
    "APD50: {results_df['APD50_s'].mean():.3f} ± {results_df['APD50_s'].std():.3f} s  \n",
    "Ratio: {results_df['APD50_90_Ratio'].mean():.3f} ± {results_df['APD50_90_Ratio'].std():.3f}\n",
    "Events: {len(results_df)}\"\"\"\n",
    "    \n",
    "    ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, \n",
    "            va='top', ha='left', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example usage with your file\n",
    "    excel_path = r\"C:\\Users\\m254292\\Downloads\\Sup Rep BeRST pilot excel.xlsx\"\n",
    "    \n",
    "    print(\"🚀 Running FIXED APD Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run analysis\n",
    "    summary_df, plots, failed_rois = run_fixed_analysis(\n",
    "        excel_path=excel_path,\n",
    "        band_factor=1.5,\n",
    "        upstroke_min=0.06,\n",
    "        photobleach_method='auto',  # Try 'polynomial', 'linear', 'rolling'\n",
    "        apply_filters=True\n",
    "    )\n",
    "    \n",
    "    if summary_df is not None:\n",
    "        print(f\"\\n✅ Analysis complete!\")\n",
    "        print(f\"   • Total events: {len(summary_df)}\")\n",
    "        print(f\"   • Total plots: {len(plots)}\")\n",
    "        \n",
    "        # Export results\n",
    "        output_path = excel_path.replace('.xlsx', '_FIXED_Results.xlsx')\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "                summary_df.to_excel(writer, sheet_name='APD_Results', index=False)\n",
    "                \n",
    "                # Add plots\n",
    "                workbook = writer.book\n",
    "                worksheet = workbook.add_worksheet('Plots')\n",
    "                \n",
    "                img_row = 0\n",
    "                for sample, roi, fig in plots[:10]:  # Limit to first 10 plots\n",
    "                    try:\n",
    "                        import io\n",
    "                        buf = io.BytesIO()\n",
    "                        fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "                        buf.seek(0)\n",
    "                        \n",
    "                        worksheet.write(img_row, 0, f\"{sample} | {roi}\")\n",
    "                        worksheet.insert_image(img_row + 1, 0, f\"{sample}_{roi}.png\",\n",
    "                                             {'image_data': buf, 'x_scale': 0.7, 'y_scale': 0.7})\n",
    "                        img_row += 35\n",
    "                        plt.close(fig)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Plot export failed for {sample}-{roi}: {e}\")\n",
    "            \n",
    "            print(f\"📁 Results exported: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Export failed: {e}\")\n",
    "            # Fallback to CSV\n",
    "            csv_path = excel_path.replace('.xlsx', '_FIXED_Results.csv')\n",
    "            summary_df.to_csv(csv_path, index=False)\n",
    "            print(f\"📁 CSV backup: {csv_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Analysis failed completely\")\n",
    "        if failed_rois:\n",
    "            print(\"Failed ROIs:\")\n",
    "            for sample, roi, reason in failed_rois[:5]:\n",
    "                print(f\"   • {sample}-{roi}: {reason}\")\n",
    "\n",
    "print(\"\\n🎉 FIXED PHOTOBLEACHING CORRECTION READY!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"  ✅ Fixed array length mismatches\")\n",
    "print(\"  ✅ Robust interpolation for missing data\") \n",
    "print(\"  ✅ Better baseline preservation\")\n",
    "print(\"  ✅ Enhanced error handling\")\n",
    "print(\"  ✅ Optimized for cardiolaminopathy studies\")\n",
    "print(\"\\nTo use: Run run_fixed_analysis() with your Excel file path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "# === After you’ve built `all_results` and `summary_df` ===\n",
    "# (and assuming you modified analyze_roi_signal to return the Figure object:)\n",
    "#\n",
    "# e.g. def analyze_roi_signal(...):\n",
    "#       …\n",
    "#       fig, ax = plt.subplots(figsize=(12,5))\n",
    "#       ax.plot(...)\n",
    "#       … \n",
    "#       return sample_name, roi_label, df_res, fig\n",
    "#\n",
    "# And in your loop you collected:\n",
    "#    plots = []  # list of (sample, roi, fig)\n",
    "#    for …:\n",
    "#        samp, roi, df_res, fig = analyze_roi_signal(...)\n",
    "#        plots.append((samp, roi, fig))\n",
    "#        # insert sample/ROI into df_res as before\n",
    "#        all_results.append(df_res)\n",
    "#\n",
    "# At this point you have:\n",
    "#    summary_df  ← pd.concat(all_results)\n",
    "#    plots       ← list of (sample, roi, fig)\n",
    "\n",
    "# === Dynamically build out_path using os.path (Option 1) ===\n",
    "folder, basename = os.path.split(excel_path)\n",
    "name, ext       = os.path.splitext(basename)\n",
    "new_name        = f\"{name} batch analysis{ext}\"\n",
    "out_path        = os.path.join(folder, new_name)\n",
    "\n",
    "# === Write summary_df and plots into the new workbook ===\n",
    "with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
    "    # 1) summary table\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "    # 2) embed plots\n",
    "    workbook  = writer.book\n",
    "    worksheet = workbook.add_worksheet('Plots')\n",
    "\n",
    "    img_row = 0\n",
    "    for sample, roi, fig in plots:\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "\n",
    "        worksheet.write(img_row, 0, f\"{sample} | {roi}\")\n",
    "        img_row += 1\n",
    "\n",
    "        worksheet.insert_image(\n",
    "            img_row, 0,\n",
    "            f\"{sample}_{roi}.png\",\n",
    "            {'image_data': buf, 'x_scale': 0.8, 'y_scale': 0.8}\n",
    "        )\n",
    "        img_row += 30\n",
    "\n",
    "print(f\"✅ Saved batch analysis to:\\n  {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: \n",
    "- Add APD50\n",
    "- Add ratio of APD50/90\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
